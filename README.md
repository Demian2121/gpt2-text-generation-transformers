# Implementación de Modelo Generativo de Lenguaje con Transformers (GPT-2)

## Descripción
Proyecto de Procesamiento de Lenguaje Natural (NLP) donde se implementa un modelo generativo de texto utilizando la arquitectura Transformer mediante la librería Hugging Face Transformers.

Se utilizó el modelo GPT-2 preentrenado para generar texto a partir de prompts personalizados, explorando parámetros como longitud máxima de secuencia y variabilidad de salida.

## Tecnologías utilizadas
- Python
- Hugging Face Transformers
- PyTorch
- Google Colab

## Funcionalidades
- Carga de modelo GPT-2 preentrenado
- Generación automática de texto
- Control de longitud de salida
- Generación basada en prompts personalizados

## Conceptos aplicados
- Modelos de lenguaje
- Arquitectura Transformer
- NLP generativo
- Inferencia con modelos preentrenados
